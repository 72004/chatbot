import streamlit as st
from huggingface_hub import InferenceClient

# Your Hugging Face API key (replace this with your actual key)
api_key = "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"

# Initialize the Inference Client with the API key
client = InferenceClient(token=api_key)

# Function that you want to include but not use
def generate_response_from_finetuned_model(tokenizer, model, base_model_response):
    input_text = f"Base Model Response:\n{base_model_response}\n\nNow refine and explain further:"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True, max_length=2048)
    
    outputs = model.generate(**inputs, max_new_tokens=100)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Streamlit App
st.title("Tutor Ai ")
st.write("Unlock Your Potential with AI Tutoring.")

# Sidebar Configuration
st.sidebar.header("Configuration")

# Check if the selected model is stored in session state
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "meta-llama/Llama-3.2-1B-Instruct"
    st.session_state.messages = []

# Model selection with reset logic
model_choice = st.sidebar.selectbox(
    "Select Model",
    options=[
        "meta-llama/Llama-3.2-1B-Instruct", 
        "google/gemma-1.1-2b-it" ,
        "microsoft/Phi-3-mini-4k-instruct",
        "mistralai/Mistral-7B-Instruct-v0.2" ,
        
    ],
    index=0,
    help="Choose between different models for chat."
)

# Reset conversation if the model is changed
if model_choice != st.session_state.selected_model:
    st.session_state.selected_model = model_choice
    st.session_state.messages = []

# Max tokens configuration
max_tokens = st.sidebar.slider("Max Tokens", 50, 1000, 500)

# Input Box
user_input = st.text_area("Enter your message:", placeholder="Your query please")

if st.button("Send"):
    if user_input.strip():
        # Append user input to conversation history
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        # Generate response from the selected model
        try:
            response = client.chat.completions.create(
                model=model_choice,
                messages=st.session_state.messages,
                max_tokens=max_tokens
            )
            # Extract model's response
            model_reply = response["choices"][0]["message"]["content"]
            st.session_state.messages.append({"role": "assistant", "content": model_reply, "model": model_choice})
        except Exception as e:
            st.error(f"An error occurred: {e}")

# Display Chat History
st.subheader("Conversation")
for message in st.session_state.messages:
    if message["role"] == "user":
        st.markdown(
            f"""
            <div style="background-color: #f2f3f4; color: black; padding: 10px; border-radius: 10px; margin-bottom: 10px;">
                <strong>You:</strong> {message['content']}
            </div>
            """,
            unsafe_allow_html=True,
        )
    elif message["role"] == "assistant":
        st.markdown(
            f"""
            <div style="background-color: #a2bc50; color: black; padding: 10px; border-radius: 10px; margin-bottom: 10px;">
                <strong>Assistant</strong> *(Response generated by `{message['model']}`)*: {message['content']}
            </div>
            """,
            unsafe_allow_html=True,
        )
